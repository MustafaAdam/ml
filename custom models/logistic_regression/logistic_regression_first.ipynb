{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd3b26b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "full_parchment_style_with_headings = \"\"\"\n",
    "<style>\n",
    "  /* Overall font and background */\n",
    "  body, .notebook-app, .container, .cell, .text_cell_render, .output_area {\n",
    "    font-family: 'Georgia', 'Palatino Linotype', Palatino, serif !important;\n",
    "    font-size: 15px !important;\n",
    "    line-height: 1.7 !important;\n",
    "    color: #3b3a32 !important;\n",
    "    background-color: #f9f6f1 !important; /* soft parchment */\n",
    "  }\n",
    "\n",
    "  /* Container */\n",
    "  .notes-container {\n",
    "    margin: 2em auto;\n",
    "    max-width: 900px;\n",
    "  }\n",
    "\n",
    "  /* Cards styled like parchment */\n",
    "  .note-card {\n",
    "    background: #fbf8f1; /* very pale parchment */\n",
    "    border: 1px solid #d3c9b7;\n",
    "    border-radius: 14px;\n",
    "    padding: 2em 2.5em;\n",
    "    margin-bottom: 2.5em;\n",
    "    box-shadow: 0 3px 10px rgba(115,100,81,0.1);\n",
    "    transition: box-shadow 0.3s ease;\n",
    "  }\n",
    "  .note-card:hover {\n",
    "    box-shadow: 0 7px 22px rgba(115,100,81,0.15);\n",
    "  }\n",
    "\n",
    "  /* Global headings outside cards */\n",
    "  h1 {\n",
    "    font-family: 'Georgia', serif;\n",
    "    font-size: 2.8em;\n",
    "    font-weight: 700;\n",
    "    color: #4b4636; /* dark olive-brown */\n",
    "    margin-top: 0;\n",
    "    margin-bottom: 0.8em;\n",
    "    border-bottom: 3px solid #c1bfae; /* soft beige underline */\n",
    "    padding-bottom: 0.4em;\n",
    "  }\n",
    "\n",
    "  h2 {\n",
    "    font-family: 'Georgia', serif;\n",
    "    font-size: 2.1em;\n",
    "    font-weight: 600;\n",
    "    color: #5a533d; /* muted olive-green */\n",
    "    margin-top: 0;\n",
    "    margin-bottom: 0.7em;\n",
    "    border-bottom: 2px solid #d3c9b7; /* lighter beige underline */\n",
    "    padding-bottom: 0.3em;\n",
    "  }\n",
    "\n",
    "  /* Headings inside cards use h3 */\n",
    "  .note-card h3 {\n",
    "    font-family: 'Georgia', serif;\n",
    "    font-size: 1.6em;\n",
    "    font-weight: 600;\n",
    "    color: #5a533d; /* muted olive-green */\n",
    "    margin-top: 0;\n",
    "    margin-bottom: 0.75em;\n",
    "    border-bottom: 2px solid #d3c9b7;\n",
    "    padding-bottom: 0.25em;\n",
    "  }\n",
    "\n",
    "  /* Paragraph text */\n",
    "  .note-card p {\n",
    "    color: #4c4b44;\n",
    "    font-size: 15px;\n",
    "    margin-bottom: 1.3em;\n",
    "  }\n",
    "\n",
    "  /* Lists */\n",
    "  .note-card ul {\n",
    "    list-style-type: disc;\n",
    "    margin-left: 1.5em;\n",
    "    padding-left: 0.5em;\n",
    "  }\n",
    "  .note-card li {\n",
    "    margin-bottom: 0.75em;\n",
    "    color: #56534b;\n",
    "    font-size: 15px;\n",
    "  }\n",
    "\n",
    "  /* Math emphasis block */\n",
    "  .math-center {\n",
    "    text-align: center;\n",
    "    font-size: 14.5px;\n",
    "    color: #706a57;\n",
    "    background: #f0ede3; /* warm parchment */\n",
    "    padding: 1em 1.5em;\n",
    "    border-radius: 12px;\n",
    "    margin-top: 1.4em;\n",
    "    margin-bottom: 1.8em;\n",
    "    font-style: italic;\n",
    "    box-shadow: inset 0 0 8px rgba(115,100,81,0.1);\n",
    "  }\n",
    "\n",
    "  /* Info callout */\n",
    "  .info {\n",
    "    display: inline-block;\n",
    "    background-color: #e4decf; /* soft warm beige */\n",
    "    color: #625d4f;\n",
    "    font-weight: 600;\n",
    "    padding: 0.4em 0.8em;\n",
    "    border-radius: 7px;\n",
    "    box-shadow: 0 1px 4px rgba(115,100,81,0.12);\n",
    "  }\n",
    "</style>\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5425bdd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mustafa/miniconda3/envs/t2/lib/python3.13/site-packages/numpy/_core/getlimits.py:551: UserWarning: Signature b'\\x00\\xd0\\xcc\\xcc\\xcc\\xcc\\xcc\\xcc\\xfb\\xbf\\x00\\x00\\x00\\x00\\x00\\x00' for <class 'numpy.longdouble'> does not match any known type: falling back to type probe function.\n",
      "This warnings indicates broken support for the dtype!\n",
      "  machar = _get_machar(dtype)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, average_precision_score\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "sns.set_palette('Spectral')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b87be0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class CustomLogisticRegression:\n",
    "    def __init__(self, learning_rate=0.01, number_of_epochs=1000,\n",
    "                 verbose=False, log_every=100):\n",
    "        self.weights_vector = None            # weights of features (coefficients). shape(n_features,)\n",
    "        self.bias = None                      # bias (intercept)\n",
    "        \n",
    "        self.learning_rate = learning_rate    # learning rate\n",
    "        self.number_of_epochs = number_of_epochs  # number of training epochs\n",
    "        \n",
    "        self.final_loss = None                # last loss value of the model\n",
    "        self.loss_history = []                # all losses generated by the model\n",
    "        \n",
    "        self.verbose = verbose                # print epoch | cost | weights norm\n",
    "        self.log_every = log_every            # log how often \n",
    "\n",
    "    def _init_parameters(self, n_features):\n",
    "        \"\"\"Initialize weights and bias.\"\"\"\n",
    "        self.weights_vector = np.zeros(n_features)\n",
    "        self.bias = 0.0\n",
    "\n",
    "    def _sigmoid(self, z):\n",
    "        \"\"\"Sigmoid activation function.\"\"\"\n",
    "        z = np.clip(z, -500, 500)  # avoid overflow\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def _log_loss(self, y, y_hat):\n",
    "        \"\"\"Compute the logistic loss.\"\"\"\n",
    "        epsilon = 1e-10\n",
    "        return -np.mean(y * np.log(y_hat + epsilon) + (1 - y) * np.log(1 - y_hat + epsilon))\n",
    "    \n",
    "    def _forward_propagation(self, X, y):\n",
    "        \"\"\"Compute predictions and loss.\"\"\"\n",
    "        z = X @ self.weights_vector + self.bias\n",
    "        y_hat = self._sigmoid(z)\n",
    "        loss = self._log_loss(y, y_hat)\n",
    "        self.loss_history.append(loss)\n",
    "        return y_hat, loss\n",
    "\n",
    "    def _back_propagation(self, X, y, y_hat):\n",
    "        \"\"\"Compute gradients.\"\"\"\n",
    "        error = y_hat - y\n",
    "        n_samples = X.shape[0]\n",
    "        weights_derivative = (X.T @ error) / n_samples\n",
    "        bias_derivative = np.mean(error)\n",
    "        return weights_derivative, bias_derivative\n",
    "\n",
    "    def _update(self, weights_derivative, bias_derivative):\n",
    "        \"\"\"Update weights and bias.\"\"\"\n",
    "        self.weights_vector -= self.learning_rate * weights_derivative\n",
    "        self.bias -= self.learning_rate * bias_derivative\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Train the logistic regression model.\"\"\"\n",
    "        self._init_parameters(X.shape[1])\n",
    "\n",
    "        for epoch in range(self.number_of_epochs):\n",
    "            y_hat, current_loss = self._forward_propagation(X, y)\n",
    "            weights_derivative, bias_derivative = self._back_propagation(X, y, y_hat)\n",
    "            self._update(weights_derivative, bias_derivative)\n",
    "\n",
    "            if self.verbose and (epoch % self.log_every == 0 or epoch == self.number_of_epochs - 1):\n",
    "                print(f\"Epoch {epoch:<5} | Loss: {current_loss:.5f} | Weights Norm: {np.linalg.norm(self.weights_vector):.5f}\")\n",
    "\n",
    "        self.final_loss = current_loss\n",
    "        return self\n",
    "    \n",
    "    def predict_probability(self, X):\n",
    "        \"\"\"Predict probabilities for input samples.\"\"\"\n",
    "        z = X @ self.weights_vector + self.bias\n",
    "        return self._sigmoid(z)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels (0 or 1).\"\"\"\n",
    "        probabilities = self.predict_probability(X)\n",
    "        return (probabilities >= 0.5).astype(int)\n",
    "\n",
    "    def plot_losses(self, ax=None):\n",
    "        \"\"\"Plot losses over epochs.\"\"\"\n",
    "        if ax is None:\n",
    "            ax = plt.gca()\n",
    "        ax.plot(self.loss_history, label=f'Final loss: {self.final_loss:.2f}')\n",
    "        ax.set_xlabel('Epochs', fontsize=12)\n",
    "        ax.set_ylabel('Loss', fontsize=12)\n",
    "        ax.set_title('Training Loss Over Epochs')\n",
    "        ax.legend()\n",
    "        ax.grid(alpha=0.5)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "t2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
